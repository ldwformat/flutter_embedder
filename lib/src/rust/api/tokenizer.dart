// This file is automatically generated, so please do not edit it.
// @generated by `flutter_rust_bridge`@ 2.11.1.

// ignore_for_file: invalid_use_of_internal_member, unused_import, unnecessary_import

import '../frb_generated.dart';
import 'package:flutter_rust_bridge/flutter_rust_bridge_for_generated.dart';

// These functions are ignored because they are not marked as `pub`: `add_special_tokens_internal`, `insert_tokenizer`, `load_tokenizer_from_json_inner`, `next_id`, `store`, `with_tokenizer_mut`, `with_tokenizer`
// These function are ignored because they are on traits that is not defined in current crate (put an empty `#[frb]` on it to unignore): `clone`, `clone`, `fmt`, `fmt`, `from`

BigInt loadTokenizerFromJson({required String json}) =>
    RustLib.instance.api.crateApiTokenizerLoadTokenizerFromJson(json: json);

BigInt loadTokenizerFromJsonWithSpecialTokens({
  required String json,
  required List<String> specialTokens,
}) => RustLib.instance.api
    .crateApiTokenizerLoadTokenizerFromJsonWithSpecialTokens(
      json: json,
      specialTokens: specialTokens,
    );

BigInt loadTokenizerFromBytes({required List<int> bytes}) =>
    RustLib.instance.api.crateApiTokenizerLoadTokenizerFromBytes(bytes: bytes);

BigInt loadTokenizerFromBytesWithSpecialTokens({
  required List<int> bytes,
  required List<String> specialTokens,
}) => RustLib.instance.api
    .crateApiTokenizerLoadTokenizerFromBytesWithSpecialTokens(
      bytes: bytes,
      specialTokens: specialTokens,
    );

BigInt loadTokenizerFromFile({required String path}) =>
    RustLib.instance.api.crateApiTokenizerLoadTokenizerFromFile(path: path);

BigInt loadTokenizerFromFileWithSpecialTokens({
  required String path,
  required List<String> specialTokens,
}) => RustLib.instance.api
    .crateApiTokenizerLoadTokenizerFromFileWithSpecialTokens(
      path: path,
      specialTokens: specialTokens,
    );

int addSpecialTokens({
  required BigInt tokenizerId,
  required List<String> tokens,
}) => RustLib.instance.api.crateApiTokenizerAddSpecialTokens(
  tokenizerId: tokenizerId,
  tokens: tokens,
);

EncodeOutput encode({
  required BigInt tokenizerId,
  required String text,
  bool? addSpecialTokens,
}) => RustLib.instance.api.crateApiTokenizerEncode(
  tokenizerId: tokenizerId,
  text: text,
  addSpecialTokens: addSpecialTokens,
);

List<EncodeOutput> encodeBatch({
  required BigInt tokenizerId,
  required List<String> texts,
  bool? addSpecialTokens,
}) => RustLib.instance.api.crateApiTokenizerEncodeBatch(
  tokenizerId: tokenizerId,
  texts: texts,
  addSpecialTokens: addSpecialTokens,
);

String decode({
  required BigInt tokenizerId,
  required List<int> ids,
  bool? skipSpecialTokens,
}) => RustLib.instance.api.crateApiTokenizerDecode(
  tokenizerId: tokenizerId,
  ids: ids,
  skipSpecialTokens: skipSpecialTokens,
);

List<String> decodeBatch({
  required BigInt tokenizerId,
  required List<Uint32List> batchIds,
  bool? skipSpecialTokens,
}) => RustLib.instance.api.crateApiTokenizerDecodeBatch(
  tokenizerId: tokenizerId,
  batchIds: batchIds,
  skipSpecialTokens: skipSpecialTokens,
);

Future<List<EncodeOutput>> encodeBatchAsync({
  required BigInt tokenizerId,
  required List<String> texts,
  bool? addSpecialTokens,
}) => RustLib.instance.api.crateApiTokenizerEncodeBatchAsync(
  tokenizerId: tokenizerId,
  texts: texts,
  addSpecialTokens: addSpecialTokens,
);

Future<List<String>> decodeBatchAsync({
  required BigInt tokenizerId,
  required List<Uint32List> batchIds,
  bool? skipSpecialTokens,
}) => RustLib.instance.api.crateApiTokenizerDecodeBatchAsync(
  tokenizerId: tokenizerId,
  batchIds: batchIds,
  skipSpecialTokens: skipSpecialTokens,
);

class EncodeOutput {
  final Uint32List ids;
  final Uint32List attentionMask;
  final Uint32List typeIds;
  final Uint32List specialTokensMask;
  final List<TokenOffsets> offsets;
  final List<String> tokens;

  const EncodeOutput({
    required this.ids,
    required this.attentionMask,
    required this.typeIds,
    required this.specialTokensMask,
    required this.offsets,
    required this.tokens,
  });

  @override
  int get hashCode =>
      ids.hashCode ^
      attentionMask.hashCode ^
      typeIds.hashCode ^
      specialTokensMask.hashCode ^
      offsets.hashCode ^
      tokens.hashCode;

  @override
  bool operator ==(Object other) =>
      identical(this, other) ||
      other is EncodeOutput &&
          runtimeType == other.runtimeType &&
          ids == other.ids &&
          attentionMask == other.attentionMask &&
          typeIds == other.typeIds &&
          specialTokensMask == other.specialTokensMask &&
          offsets == other.offsets &&
          tokens == other.tokens;
}

class TokenOffsets {
  final int start;
  final int end;

  const TokenOffsets({required this.start, required this.end});

  @override
  int get hashCode => start.hashCode ^ end.hashCode;

  @override
  bool operator ==(Object other) =>
      identical(this, other) ||
      other is TokenOffsets &&
          runtimeType == other.runtimeType &&
          start == other.start &&
          end == other.end;
}
